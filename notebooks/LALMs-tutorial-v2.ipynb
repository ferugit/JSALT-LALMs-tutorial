{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b438be15-773a-4be0-ad20-839b3b31836a",
   "metadata": {},
   "source": [
    "# Introduction to Large Audio Language Models\n",
    "\n",
    "**Laboratory session: AuGI - Towards audio general intelligence**\n",
    "\n",
    "September 5th, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a4d29-8945-4aa5-97f7-86ff31fa415e",
   "metadata": {},
   "source": [
    "## Introduction and Objectives\n",
    "\n",
    "1. Setting up the Environment\n",
    "2. Exploring CLAP text-audio encoder\n",
    "3. Exploring Audio Flamingo 2\n",
    "4. Exploring MMAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce8ecb-8403-4fb3-8e0c-f9cf3ed7eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ferugit/JSALT-LALMs-tutorial.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db744ee1-fcf5-4583-8704-b14d85bc73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "#%cd ..\n",
    "\n",
    "# Remote\n",
    "%cd JSALT-LALMs-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c58aa91-aef6-47ef-902a-af1b1145a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbbeff3-db07-4937-aa39-2132a0129ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Qwen2.5-0.5B model\n",
    "!./download_hf_model.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e7be6-ddf5-48cd-8c96-4f5b4e358965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download AF2 model: CLAP encoder, Audio Transformer and XATTN\n",
    "!./download_af2.sh \"YOUR_HF_TOKEN_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf4bc9-01e0-49e9-ab33-304ea7b73bcf",
   "metadata": {},
   "source": [
    "# CLAP: Audio Encoder\n",
    "\n",
    "![CLAP Architecture](../assets/clap-arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e6847-02ab-47da-806f-9e9709d0e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import umap\n",
    "import IPython\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "#from src.audio_flamingo_2.my_laion_clap.CLAP.src import laion_clap as local_clap\n",
    "import laion_clap\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2aacc3-734b-4a0d-9a21-c3a504e16ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model = laion_clap.CLAP_Module(enable_fusion=False)\n",
    "model.load_ckpt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4438a0fa-9d2e-4d15-9957-e665d72eabe0",
   "metadata": {},
   "source": [
    "## Zero-shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25abf2-54df-430d-a964-f480b73a2e49",
   "metadata": {},
   "source": [
    "![CLAP Architecture](../assets/zero-shot_classification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c5db3-3118-41bf-9c7a-f102aa85a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's listen some audios\n",
    "cat_filename = \"assets/cat.wav\"\n",
    "dog_filename = \"assets/dog_barking.wav\"\n",
    "another_dog_filename = \"assets/dog.wav\"\n",
    "breaking_filename = \"assets/breaking.wav\"\n",
    "cough_filename = \"assets/cough.wav\"\n",
    "music_filename = \"assets/dance_matisse_musiclm.wav\"\n",
    "role_filename = \"assets/role.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b9dfc7-1b23-47bf-87c5-22929eb59bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(cat_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e34e4-fcd7-4854-84e7-a44a98954b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(dog_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c5a410-f017-4d80-95ff-bd8a3c773f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get audio embeddings from audio files\n",
    "audio_file = [cat_filename, dog_filename]\n",
    "with torch.no_grad():\n",
    "    audio_embed = model.get_audio_embedding_from_filelist(x = audio_file, use_tensor=True)\n",
    "print(audio_embed[:,-20:])\n",
    "print(audio_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33308745-fd61-4f4e-9da0-3e539e8df6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text embedings from texts\n",
    "text_data = [\"This is a sound of a dog\", \"This is a sound of a cat\"] \n",
    "with torch.no_grad():\n",
    "    text_embed = model.get_text_embedding(text_data, use_tensor=True)\n",
    "print(text_embed[:,-20:])\n",
    "print(text_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b552259-a0cd-4945-970b-550bf0129409",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = audio_embed @ text_embed.t()\n",
    "print(\"Similarity matrix:\\n\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368994d-a500-4289-be23-b5d9c752ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate embeddings\n",
    "embeddings = torch.cat([audio_embed, text_embed], dim=0).cpu().numpy()\n",
    "labels = ['audio_cat', 'audio_dog', 'text_dog', 'text_cat']\n",
    "\n",
    "# Solve UMAP 2D projection\n",
    "reducer = umap.UMAP(n_neighbors=2, random_state=1)\n",
    "embeddings_2d = reducer.fit_transform(embeddings)\n",
    "\n",
    "# Plot emdedding distances\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], label=label)\n",
    "    plt.text(embeddings_2d[i, 0]+0.01, embeddings_2d[i, 1]+0.01, label)\n",
    "\n",
    "# Draw lines between audio and text pairs to show distances\n",
    "plt.plot([embeddings_2d[0, 0], embeddings_2d[3, 0]], [embeddings_2d[0, 1], embeddings_2d[3, 1]], 'r--', label='cat distance')\n",
    "plt.plot([embeddings_2d[1, 0], embeddings_2d[2, 0]], [embeddings_2d[1, 1], embeddings_2d[2, 1]], 'b--', label='dog distance')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('2D Visualization of Audio and Text Embeddings with Distances')\n",
    "plt.xlabel('UMAP-1')\n",
    "plt.ylabel('UMAP-2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3e4b3-926e-4ee3-a901-a83f4edddbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "cos_sim = torch.nn.CosineSimilarity(dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264f14f-ff8a-4b59-a910-fc313ff0d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cacluclate cosine distance\n",
    "audio_file = [dog_filename]\n",
    "with torch.no_grad():\n",
    "    audio_embed = model.get_audio_embedding_from_filelist(x = audio_file, use_tensor=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embed = model.get_text_embedding(\"This is a dog barking\", use_tensor=True)\n",
    "\n",
    "similarity = cos_sim(audio_embed[-1], text_embed[-1])\n",
    "distance = 1 - similarity\n",
    "print(\"Cosine Distance:\", distance.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f90a5-1ecb-419e-aa3f-de88fbb7bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(cough_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7ea48-9c30-4270-87cb-cff5a2578311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cacluclate cosine distance\n",
    "audio_file = [cough_filename]\n",
    "with torch.no_grad():\n",
    "    audio_embed = model.get_audio_embedding_from_filelist(x = audio_file, use_tensor=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embed = model.get_text_embedding(\"This is a dog barking\", use_tensor=True)\n",
    "\n",
    "similarity = cos_sim(audio_embed[-1], text_embed[-1])\n",
    "distance = 1 - similarity\n",
    "print(\"Cosine Distance:\", distance.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c293c8-2e82-49ce-a78b-08b1cb538b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some vars\n",
    "del model\n",
    "del audio_embed\n",
    "del text_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95e4c8-3bc9-4a19-89fe-6b3521f626d2",
   "metadata": {},
   "source": [
    "# Audio Flamingo 2\n",
    "\n",
    "![AF2 Architecture](../assets/af2_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d420aa5-8aba-4d84-b746-d3745e9702fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat src/audio_flamingo_2/config/inference_2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6de94-c53e-45da-94de-ffc2a6d62be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat run_af2_single_inference.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f1039-0f53-42b1-8917-e497900dc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_af2_single_inference.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b80022-01de-4cb1-9031-c4fbe425af34",
   "metadata": {},
   "source": [
    "## Now load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0dcfb-2264-41c2-8e30-c875248fe3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import src.audio_flamingo_2.factory as factory\n",
    "from src.audio_flamingo_2.inference_utils import read_audio, load_audio, predict, get_num_windows\n",
    "from src.audio_flamingo_2.utils import Dict2Class, float32_to_int16, int16_to_float32, get_autocast, get_cast_dtype\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661271bb-4d58-4252-a651-6cd717bee1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "config = yaml.load(open(\"src/audio_flamingo_2/config/inference_2.yaml\"), Loader=yaml.FullLoader)\n",
    "\n",
    "#print(config)\n",
    "data_config = config['data_config']\n",
    "model_config = config['model_config']\n",
    "clap_config = config['clap_config']\n",
    "model_args = Dict2Class(config['train_config'])\n",
    "\n",
    "# Cast the model to the appropriate dtype\n",
    "autocast = get_autocast(\n",
    "    model_args.precision, cache_enabled=(not model_args.fsdp)\n",
    ")\n",
    "cast_dtype = get_cast_dtype(model_args.precision)\n",
    "\n",
    "# Get the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set Hugging Face cache directory\n",
    "model, tokenizer = factory.create_model_and_transforms(\n",
    "    **model_config,\n",
    "    clap_config=clap_config, \n",
    "    use_local_files=True,\n",
    "    gradient_checkpointing=False,\n",
    "    freeze_lm_embeddings=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer created successfully.\")\n",
    "\n",
    "print(\"Loading trained weights...\")\n",
    "\n",
    "# CLAP, tokenizer and LLM are pretrained. \n",
    "# XATTN and Transformer are not. We need to load the pretrained weights.\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the pretrained weights\n",
    "ckpt_path = config['inference_config']['pretrained_path']\n",
    "metadata_path = os.path.join(ckpt_path, \"safe_ckpt/metadata.json\")\n",
    "\n",
    "# Load metadata\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Reconstruct the full state_dict\n",
    "state_dict = {}\n",
    "\n",
    "# Load each SafeTensors chunk\n",
    "for chunk_name in metadata:\n",
    "    chunk_path = f\"safe_ckpt/{chunk_name}.safetensors\"\n",
    "    chunk_tensors = load_file(os.path.join(ckpt_path, chunk_path))\n",
    "\n",
    "    # Merge tensors into state_dict\n",
    "    state_dict.update(chunk_tensors)\n",
    "\n",
    "missing_keys, unexpected_keys = model.load_state_dict(state_dict, False)\n",
    "\n",
    "print(\"Missing keys:\", missing_keys)\n",
    "print(\"Unexpected keys:\", unexpected_keys)\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ffd41-c0e0-4816-8580-06ddab7e8549",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding = {\n",
    "    \"do_sample\": False,  # Set to True for sampling, False for greedy/beam search\n",
    "    \"temperature\": 0.0,\n",
    "    \"num_beams\": 1,\n",
    "    \"top_k\": 30,\n",
    "    \"top_p\": 0.95,\n",
    "    \"num_return_sequences\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "question =  \"What is the gender of the person?\"\n",
    "\n",
    "# Perform inference\n",
    "result = predict(\n",
    "    cough_filename,\n",
    "    question,\n",
    "    clap_config,\n",
    "    inference_kwargs=decoding,\n",
    "    cast_dtype=cast_dtype,\n",
    "    device=device,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")\n",
    "print(\"Inference completed.\\n\\n\")\n",
    "print(\"*\" * 50)\n",
    "print(\"Prompt:\", question)\n",
    "print(\"Audio path:\", cough_filename)\n",
    "print(\"Inference result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc329e-315d-480e-a660-664bc2503634",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding = {\n",
    "    \"do_sample\": False,  # Set to True for sampling, False for greedy/beam search\n",
    "    \"temperature\": 0.0,\n",
    "    \"num_beams\": 1,\n",
    "    \"top_k\": 30,\n",
    "    \"top_p\": 0.95,\n",
    "    \"num_return_sequences\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "question =  \"Is the person ill?\"\n",
    "\n",
    "# Perform inference\n",
    "result = predict(\n",
    "    cough_filename,\n",
    "    question,\n",
    "    clap_config,\n",
    "    inference_kwargs=decoding,\n",
    "    cast_dtype=cast_dtype,\n",
    "    device=device,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")\n",
    "print(\"Inference completed.\\n\\n\")\n",
    "print(\"*\" * 50)\n",
    "print(\"Prompt:\", question)\n",
    "print(\"Audio path:\", cough_filename)\n",
    "print(\"Inference result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f96575-5c58-4136-9e1b-41ef266f9b00",
   "metadata": {},
   "source": [
    "# MMAU Benchmark\n",
    "\n",
    "![AF2 Architecture](../assets/mmau_hero.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dc631d-2a1d-48c4-8bd1-e00c1578a16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding = {\n",
    "    \"do_sample\": False,  # Set to True for sampling, False for greedy/beam search\n",
    "    \"temperature\": 0.0,\n",
    "    \"num_beams\": 1,\n",
    "    \"top_k\": 30,\n",
    "    \"top_p\": 0.95,\n",
    "    \"num_return_sequences\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "question =  \"\"\"How are the two speakers connected?\n",
    "      (A). rental agent-tenant\n",
    "      (B). curator-artist\n",
    "      (C). author-editor\n",
    "      (D). flight instructor-student pilot\"\"\"\n",
    "\n",
    "# Perform inference\n",
    "result = predict(\n",
    "    cough_filename,\n",
    "    question,\n",
    "    clap_config,\n",
    "    inference_kwargs=decoding,\n",
    "    cast_dtype=cast_dtype,\n",
    "    device=device,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")\n",
    "print(\"Inference completed.\\n\\n\")\n",
    "print(\"*\" * 50)\n",
    "print(\"Prompt:\", question)\n",
    "print(\"Audio path:\", cough_filename)\n",
    "print(\"Inference result:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
