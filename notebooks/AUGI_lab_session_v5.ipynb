{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qncY3FktdgMI"
      },
      "source": [
        "# JSALT 2025 - Introduction to Large Audio Language Models\n",
        "\n",
        "**Laboratory session: AuGI - Towards audio general intelligence**\n",
        "\n",
        "June 20th, 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6yNO58J-7my"
      },
      "source": [
        "## Introduction and Objectives\n",
        "\n",
        "In this notebook, we will explore Large Audio Language Models (LALMs) and their use for inference and answering questions.\n",
        "\n",
        "In particular, we will use the recently published Audio Flamingo 2 (AF2) model (based on the CLAP audio encoder) and the MMAU dataset.\n",
        "\n",
        "üìñ **Some papers**\n",
        "* [CLAP](https://arxiv.org/pdf/2211.06687)\n",
        "* [AudioFlamingo 2](https://arxiv.org/pdf/2503.03983)\n",
        "* [MMAU dataset](https://openreview.net/pdf?id=TeVAZXr3yv)\n",
        "\n",
        "\n",
        "## Outline\n",
        "\n",
        "Through this notebook, we will:\n",
        "\n",
        "1. Set up the Python environment\n",
        "2. Explore the CLAP audio encoder and its embeddings\n",
        "3. Use AudioFlamingo 2 (AF2) for inference\n",
        "4. Explore the MMAU dataset\n",
        "5. Prepare some AQA to test the model further\n",
        "\n",
        "\n",
        "\n",
        "## Materials\n",
        "\n",
        "The materials needed for this session are in the followign GitHub repository:\n",
        "\n",
        "https://github.com/ferugit/JSALT-LALMs-tutorial.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1BzhOOn-6H4"
      },
      "source": [
        "# 1. Set up the Python environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiWqca8brveq"
      },
      "source": [
        "In order to prepare the libraries needed, we will first clone the code from the GitHub repository and install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remote\n",
        "!git clone https://github.com/ferugit/JSALT-LALMs-tutorial.git\n",
        "%cd JSALT-LALMs-tutorial"
      ],
      "metadata": {
        "id": "qTdE2EpELc11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "sLPivnb-Ud19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Explore the CLAP audio encoder and its embeddings"
      ],
      "metadata": {
        "id": "1vHbvMFwvUJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üëè CLAP (Contrastive Language-Audio Pretraining)\n",
        "\n",
        "CLAP is a dual encoder model that learns joint representations of audio and language through contrastive learning, similar to CLIP (Contrastive Language‚ÄìImage Pretraining).\n",
        "\n"
      ],
      "metadata": {
        "id": "-IVUSJsnvYAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üõ†Ô∏è Architecture\n",
        "\n",
        "1. **Audio Encoder**\n",
        "   - Based on a CNN or transformer-based model (e.g. PANN and HTSAT).\n",
        "   - Takes log-mel spectrograms as input and produces an embedding vector.\n",
        "   - Trained to capture the semantic content of audio.\n",
        "\n",
        "2. **Text Encoder**\n",
        "   - A transformer (e.g. CLIP, BERT or RoBERTa).\n",
        "   - Encodes natural language prompts or captions into a dense representation.\n",
        "\n",
        "3. **Projection Layers**\n",
        "   - Both audio and text embeddings are projected into the same latent space.\n",
        "\n",
        "4. **Contrastive Loss (InfoNCE)**\n",
        "   - During training, matched audio-text pairs are pulled together in embedding space, and mismatched ones are pushed apart.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/LAION-AI/CLAP/main/assets/audioclip-arch.png\" width=\"800\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "OjNCV1l_vqFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To explore the CLAP model, we will first import some python libraries and download the trained model:"
      ],
      "metadata": {
        "id": "UwSXFLhcv4Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import umap\n",
        "import IPython\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "\n",
        "import laion_clap\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TenWeFr7v-6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = laion_clap.CLAP_Module(enable_fusion=False)\n",
        "model.load_ckpt()"
      ],
      "metadata": {
        "id": "jjpOKN_vwFh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This model could be use for _zero-shot classification_:\n",
        "\n",
        "![CLAP Architecture](../assets/zero-shot_classification.png)\n",
        "<img src=\"https://raw.githubusercontent.com/ferugit/JSALT-LALMs-tutorial/blob/master/assets/zero-shot_classification.png\" width=\"800\"/>"
      ],
      "metadata": {
        "id": "PNfVMjmWwS8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For that, we can use the audios included in the _assets_ directory of the repository.\n",
        "\n",
        "You can listen to some of these audios:"
      ],
      "metadata": {
        "id": "FO58fl3Q0Exg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_filename = \"assets/cat.wav\"\n",
        "dog_filename = \"assets/dog_barking.wav\"\n",
        "another_dog_filename = \"assets/dog.wav\"\n",
        "breaking_filename = \"assets/breaking.wav\"\n",
        "cough_filename = \"assets/cough.wav\"\n",
        "music_filename = \"assets/dance_matisse_musiclm.wav\""
      ],
      "metadata": {
        "id": "2r5U9DLk0Vcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio(cat_filename)"
      ],
      "metadata": {
        "id": "3YaNr0_F0Wxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio(dog_filename)"
      ],
      "metadata": {
        "id": "f4lIEW8F0bOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, you can obtain the embeddings from the CLAP audio encoder like this:"
      ],
      "metadata": {
        "id": "nkXf9k_30b8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get audio embeddings from audio files\n",
        "audio_file = [cat_filename, dog_filename]\n",
        "with torch.no_grad():\n",
        "    audio_embed = model.get_audio_embedding_from_filelist(x = audio_file, use_tensor=True)\n",
        "print(audio_embed[:,-20:])\n",
        "print(audio_embed.shape)"
      ],
      "metadata": {
        "id": "qFCZKxo70si_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, in a similar way, you can get the text embeddings from a given description using the CLAP model:"
      ],
      "metadata": {
        "id": "WFfS7XpH0zOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get text embedings from texts\n",
        "text_data = [\"This is a sound of a dog\", \"This is a sound of a cat\"]\n",
        "with torch.no_grad():\n",
        "    text_embed = model.get_text_embedding(text_data, use_tensor=True)\n",
        "print(text_embed[:,-20:])\n",
        "print(text_embed.shape)"
      ],
      "metadata": {
        "id": "JCBiLgAC0uKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This way, you can estimate a similarity matrix from the audio and text embeddings:"
      ],
      "metadata": {
        "id": "T5E5e4oL1Be_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = audio_embed @ text_embed.t()\n",
        "print(\"Similarity matrix:\\n\", similarity)"
      ],
      "metadata": {
        "id": "YakPp2tl08Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to visualize the embeddings in a 2D space, we can project them and see how close they are depending on the audio and text chosen:"
      ],
      "metadata": {
        "id": "ZgADH3L81L9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate embeddings\n",
        "embeddings = torch.cat([audio_embed, text_embed], dim=0).cpu().numpy()\n",
        "labels = ['audio_cat', 'audio_dog', 'text_dog', 'text_cat']\n",
        "\n",
        "# Solve UMAP 2D projection\n",
        "reducer = umap.UMAP(n_neighbors=2, random_state=1)\n",
        "embeddings_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "# Plot emdedding distances\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for i, label in enumerate(labels):\n",
        "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], label=label)\n",
        "    plt.text(embeddings_2d[i, 0]+0.01, embeddings_2d[i, 1]+0.01, label)\n",
        "\n",
        "# Draw lines between audio and text pairs to show distances\n",
        "plt.plot([embeddings_2d[0, 0], embeddings_2d[3, 0]], [embeddings_2d[0, 1], embeddings_2d[3, 1]], 'r--', label='cat distance')\n",
        "plt.plot([embeddings_2d[1, 0], embeddings_2d[2, 0]], [embeddings_2d[1, 1], embeddings_2d[2, 1]], 'b--', label='dog distance')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('2D Visualization of Audio and Text Embeddings with Distances')\n",
        "plt.xlabel('UMAP-1')\n",
        "plt.ylabel('UMAP-2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p8AKWXJG1b0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here there are some examples of similarity and distances (cosine) for different audios:"
      ],
      "metadata": {
        "id": "yK8J78BC1oe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine Similarity\n",
        "cos_sim = torch.nn.CosineSimilarity(dim=0)"
      ],
      "metadata": {
        "id": "Bm-fOMJB1fAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cacluclate cosine distance\n",
        "audio_file = [dog_filename]\n",
        "with torch.no_grad():\n",
        "    audio_embed = model.get_audio_embedding_from_filelist(x = audio_file, use_tensor=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_embed = model.get_text_embedding(\"This is a dog barking\", use_tensor=True)\n",
        "\n",
        "similarity = cos_sim(audio_embed[-1], text_embed[-1])\n",
        "distance = 1 - similarity\n",
        "print(\"Cosine Distance:\", distance.item())"
      ],
      "metadata": {
        "id": "9F9mvhQ914u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio(cough_filename)"
      ],
      "metadata": {
        "id": "9Nq4d92V15hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cacluclate cosine distance\n",
        "audio_file = [cough_filename]\n",
        "with torch.no_grad():\n",
        "    audio_embed = model.get_audio_embedding_from_filelist(x = audio_file, use_tensor=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_embed = model.get_text_embedding(\"This is a dog barking\", use_tensor=True)\n",
        "\n",
        "similarity = cos_sim(audio_embed[-1], text_embed[-1])\n",
        "distance = 1 - similarity\n",
        "print(\"Cosine Distance:\", distance.item())"
      ],
      "metadata": {
        "id": "_6DsNDOv19Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Use AudioFlamingo 2 (AF2) for inference\n"
      ],
      "metadata": {
        "id": "-vbMHVU8bXb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü¶© AudioFlamingo 2\n",
        "\n",
        "AudioFlamingo 2 (AF2) is a state-of-the-art Audio-Language Model (ALM) wit advanced audio understanding and reasoning capabilities.\n",
        "This model is capable of handling tasks like:\n",
        "- **Audio Captioning**: \"Describe what you hear.\"\n",
        "- **Sound Event Detection**: \"Is there a siren in this audio?\"\n",
        "- **Question Answering with Audio**: \"Which is the mood of the second speaker?\"\n",
        "\n",
        "\n",
        "> **Note**: This notebook is designed for inference only."
      ],
      "metadata": {
        "id": "1gl_JooI2lJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üõ†Ô∏è Architecture\n",
        "\n",
        "1. **AF-CLAP**: CLAP-based audio encoder with sliding window feature extraction\n",
        "\n",
        "2. **Transformation Layers** Expand and project audio features to a dimension compatible with the LLM.\n",
        "\n",
        "3. **Frozen Language Model (LLM)**: The decoder-only casual LLM (Qwen2.5-3B).\n",
        "\n",
        "4. **Gated Cross-Attention**: XATTN-Dense layers for audio conditioning.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NVIDIA/audio-flamingo/main/assets/af2_arch.png\" width=\"1500\"/>"
      ],
      "metadata": {
        "id": "L8jJhsw-2pew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use it for inference, first of all, we need to download the Qwen model (0.5B version in this case) from HuggingFace and the AF2 checkpoint (you will need to use your HF token as argument to the script):"
      ],
      "metadata": {
        "id": "IVtrIZ8PbfeX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUWbp268rKg1"
      },
      "outputs": [],
      "source": [
        "# Download Qwen2.5-0.5B model\n",
        "!./download_hf_model.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download AF2 model: CLAP encoder, Audio Transformer and XATTN\n",
        "!./download_af2.sh \"YOUR_HF_TOKEN_HERE\""
      ],
      "metadata": {
        "id": "msS_Adrm3aFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls models/"
      ],
      "metadata": {
        "id": "EG0yZwnpR5JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can run the inference over one file (check the script to run the inference as well as the config file):"
      ],
      "metadata": {
        "id": "hrOqd4EMcBEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat run_af2_single_inference.sh"
      ],
      "metadata": {
        "id": "J3LGtNTeYhEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat src/audio_flamingo_2/config/inference.yaml"
      ],
      "metadata": {
        "id": "c-j7JouzxhJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./run_af2_single_inference.sh"
      ],
      "metadata": {
        "id": "t7rvP6q0XyAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can listen to the audio used for inference in this case and check if the model outputs a good description."
      ],
      "metadata": {
        "id": "ZvyAW7pe39wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio(music_filename)"
      ],
      "metadata": {
        "id": "gkfYxupF4FOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Explore the MMAU dataset\n",
        "\n",
        "## üóÇÔ∏è MMAU: Multimodal Audio Understanding Dataset\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/Sakshi113/MMAU/main/mmau_intro.jpg\" width=\"800\"/>\n",
        "\n",
        "\n",
        "**MMAU** (Multimodal Audio Understanding) is a benchmark dataset introduced alongside AudioFlamingo 2 to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning.\n",
        "\n",
        "- **10k curated audio clips** paired with human annotated natural language questions and answers.\n",
        "- **3 major audio domains**: Speech, sounds and music.\n",
        "- **27 Diverse task**: 16 reasoning and 11 information extraction tasks.\n",
        "\n",
        "- **test_mini set**: 1000 questions. Reflects the task distribution of the main test set and is intended for hyperparameter tuning."
      ],
      "metadata": {
        "id": "wOPhP_8mcHiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Task Examples\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/Sakshi113/MMAU/main/mmau_hero.jpg\" width=\"1000\"/>"
      ],
      "metadata": {
        "id": "Bif_8UBZFrVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the following code, you can compute some statistics about the composition of the dataset and explore it:"
      ],
      "metadata": {
        "id": "pjffWfMw5F1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import scienceplots\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use(['science', 'nature', 'bright', 'no-latex'])\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)\n",
        "plt.rc('axes', titlesize=10)"
      ],
      "metadata": {
        "id": "faJTllN-Fslx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "qUA-wo22GZ6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mini_file = 'mmau-test-mini.json'\n",
        "\n",
        "with open(test_mini_file, 'r') as f:\n",
        "    test_mini_json = json.load(f)\n",
        "\n",
        "test_mini_df = pd.DataFrame(test_mini_json)"
      ],
      "metadata": {
        "id": "EhE1JH4_GR3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mini_df"
      ],
      "metadata": {
        "id": "t7rrulG1H6i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mini_df.describe()\n"
      ],
      "metadata": {
        "id": "wfv6juvmIBgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mini_df['category'].value_counts()"
      ],
      "metadata": {
        "id": "pOFry6F-I2Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "test_mini_df[\"task\"].value_counts().plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    labeldistance=None,\n",
        "    startangle=20,\n",
        "    pctdistance=1.18,\n",
        "    textprops={'fontsize': 10}\n",
        ")\n",
        "plt.title(\"Task\")\n",
        "plt.ylabel(\"\")\n",
        "plt.legend(loc=4, ncol=1, frameon=True, framealpha=1.0, bbox_to_anchor=(1.2, 0.4), prop={'size': 10})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yhZ7ompaMcbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "test_mini_df[\"category\"].value_counts().plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    labeldistance=None,\n",
        "    startangle=20,\n",
        "    pctdistance=1.18,\n",
        "    textprops={'fontsize': 10}\n",
        ")\n",
        "plt.title(\"Category\")\n",
        "plt.ylabel(\"\")\n",
        "plt.legend(loc=4, ncol=1, frameon=True, framealpha=1.0, bbox_to_anchor=(1.55, 0.4), prop={'size': 10})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DXfbJrjfMhIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 14 faded colors using seaborn's color palette and add transparency\n",
        "colors = sns.color_palette(\"colorblind\", 14)  # Use any palette: husl, pastel, muted, etc.\n",
        "faded_colors = [(r, g, b, 0.6) for r, g, b in colors]  # Add alpha = 0.6 for faded effect\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "test_mini_df[\"dataset\"].value_counts().plot.pie(\n",
        "    colors=faded_colors,\n",
        "    autopct='%1.1f%%',\n",
        "    labeldistance=None,\n",
        "    startangle=20,\n",
        "    pctdistance=1.18,\n",
        "    textprops={'fontsize': 10}\n",
        ")\n",
        "plt.title(\"Dataset\")\n",
        "plt.ylabel(\"\")\n",
        "plt.legend(loc=4, ncol=1, frameon=True, framealpha=1.0,\n",
        "           bbox_to_anchor=(1.55, 0.1), prop={'size': 10})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P_8hnZtOMkjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_mini_df['sub-category'].unique())"
      ],
      "metadata": {
        "id": "5eCfUyfrMoKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "test_mini_df['sub-category'].value_counts().plot(kind='barh')\n",
        "plt.ylabel(\"\")\n",
        "plt.title('Sub-categories in MMAU test-mini')"
      ],
      "metadata": {
        "id": "DqB1P17wMqyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mini_df[\"num_choices\"] = test_mini_df[\"choices\"].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "J-cdi8uLM1Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "test_mini_df[\"num_choices\"].value_counts().plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    labeldistance=None,\n",
        "    startangle=20,\n",
        "    pctdistance=1.18,\n",
        "    textprops={'fontsize': 10}\n",
        ")\n",
        "plt.title(\"Number of choices\")\n",
        "plt.ylabel(\"\")\n",
        "plt.legend(loc=4, ncol=1, frameon=True, framealpha=1.0, bbox_to_anchor=(1.2, 0.4), prop={'size': 10})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7WBp3rzvM3lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mini_df[test_mini_df[\"num_choices\"] == 2]"
      ],
      "metadata": {
        "id": "JSEY5W9aM7w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mini_df[test_mini_df[\"sub-category\"] == \"Phonemic Stress Pattern Analysis\"].sample(1)['question'].item()"
      ],
      "metadata": {
        "id": "J6rXznZZNAwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "oxvm2Ra1QNU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manifest_file = 'manifest_mmau-test-mini.json'\n",
        "with open(manifest_file, 'r') as f:\n",
        "    manifest_file = json.load(f)\n",
        "\n",
        "len_df = pd.DataFrame(manifest_file['data']).T\n",
        "len_df"
      ],
      "metadata": {
        "id": "rG-krsKTQJBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "len_df['duration'].plot.kde()\n",
        "plt.title(\"Length distribution\")\n",
        "plt.ylabel(\"\")\n",
        "#plt.legend(loc=4, ncol=1, frameon=True, framealpha=1.0, bbox_to_anchor=(1.2, 0.4), prop={'size': 10})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H_5YpBbCQd0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Prepare some AQA to test the model further"
      ],
      "metadata": {
        "id": "5AriPl_P6kx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to do some inference over different files, as well as preparing some other questions, you can first load the model:"
      ],
      "metadata": {
        "id": "JdLQ1eXn6usl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "import src.audio_flamingo_2.factory as factory\n",
        "from src.audio_flamingo_2.inference_utils import read_audio, load_audio, predict, get_num_windows\n",
        "from src.audio_flamingo_2.utils import Dict2Class, float32_to_int16, int16_to_float32, get_autocast, get_cast_dtype\n",
        "from safetensors.torch import load_file"
      ],
      "metadata": {
        "id": "tCMkGFZf6mZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the config file\n",
        "config = yaml.load(open(\"src/audio_flamingo_2/config/inference.yaml\"), Loader=yaml.FullLoader)\n",
        "\n",
        "data_config = config['data_config']\n",
        "model_config = config['model_config']\n",
        "clap_config = config['clap_config']\n",
        "model_args = Dict2Class(config['train_config'])\n",
        "\n",
        "# Cast the model to the appropriate dtype\n",
        "autocast = get_autocast(\n",
        "    model_args.precision, cache_enabled=(not model_args.fsdp)\n",
        ")\n",
        "cast_dtype = get_cast_dtype(model_args.precision)\n",
        "\n",
        "# Get the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set Hugging Face cache directory\n",
        "model, tokenizer = factory.create_model_and_transforms(\n",
        "    **model_config,\n",
        "    clap_config=clap_config,\n",
        "    use_local_files=True,\n",
        "    gradient_checkpointing=False,\n",
        "    freeze_lm_embeddings=True,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "print(\"Model and tokenizer created successfully.\")\n",
        "\n",
        "print(\"Loading trained weights...\")\n",
        "\n",
        "# CLAP, tokenizer and LLM are pretrained.\n",
        "# XATTN and Transformer are not. We need to load the pretrained weights.\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load the pretrained weights\n",
        "ckpt_path = config['inference_config']['pretrained_path']\n",
        "metadata_path = os.path.join(ckpt_path, \"safe_ckpt/metadata.json\")\n",
        "\n",
        "# Load metadata\n",
        "with open(metadata_path, \"r\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "# Reconstruct the full state_dict\n",
        "state_dict = {}\n",
        "\n",
        "# Load each SafeTensors chunk\n",
        "for chunk_name in metadata:\n",
        "    chunk_path = f\"safe_ckpt/{chunk_name}.safetensors\"\n",
        "    chunk_tensors = load_file(os.path.join(ckpt_path, chunk_path))\n",
        "\n",
        "    # Merge tensors into state_dict\n",
        "    state_dict.update(chunk_tensors)\n",
        "\n",
        "missing_keys, unexpected_keys = model.load_state_dict(state_dict, False)\n",
        "\n",
        "print(\"Missing keys:\", missing_keys)\n",
        "print(\"Unexpected keys:\", unexpected_keys)\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "id": "02OF0yzT62fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, you can do the decoding to get the results of the model:"
      ],
      "metadata": {
        "id": "Gcro5OJs7SJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoding = {\n",
        "    \"do_sample\": False,  # Set to True for sampling, False for greedy/beam search\n",
        "    \"temperature\": 0.0,\n",
        "    \"num_beams\": 1,\n",
        "    \"top_k\": 30,\n",
        "    \"top_p\": 0.95,\n",
        "    \"num_return_sequences\": 1,\n",
        "}\n",
        "\n",
        "\n",
        "question =  \"YOUR_QUESTION_HERE\" # example: \"What is the gender of the person\"\n",
        "\n",
        "# Perform inference\n",
        "result = predict(\n",
        "    cough_filename,\n",
        "    question,\n",
        "    clap_config,\n",
        "    inference_kwargs=decoding,\n",
        "    cast_dtype=cast_dtype,\n",
        "    device=device,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model\n",
        ")\n",
        "print(\"Inference completed.\\n\\n\")\n",
        "print(\"*\" * 50)\n",
        "print(\"Prompt:\", question)\n",
        "print(\"Audio path:\", cough_filename)\n",
        "print(\"Inference result:\", result)"
      ],
      "metadata": {
        "id": "9UX_x-KP7HMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}